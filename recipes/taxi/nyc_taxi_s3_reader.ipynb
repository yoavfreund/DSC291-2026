{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC Taxi Data S3 Reader\n",
        "\n",
        "This notebook demonstrates how to read NYC taxi data from S3 using Dask for efficient processing of large datasets.\n",
        "\n",
        "## Data Source\n",
        "- **Dataset**: NYC Taxi & Limousine Commission (TLC) data\n",
        "- **S3 Location**: `s3://nyc-tlc/trip data/`\n",
        "- **Format**: Parquet files\n",
        "- **Years Available**: 2009-2023\n",
        "\n",
        "## Features\n",
        "- Lazy loading of large datasets\n",
        "- Parallel processing with Dask\n",
        "- Memory-efficient operations\n",
        "- Data exploration and analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting NYC Taxi Data Analysis at 2025-10-17 16:16:54\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "print(f\"Starting NYC Taxi Data Analysis at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Explore Available Data\n",
        "\n",
        "First, let's see what data is available in the S3 bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exploring bucket: s3://nyc-tlc/trip data/\n",
            "Error exploring S3 bucket: Access Denied\n",
            "This might be due to network connectivity or bucket access restrictions\n"
          ]
        }
      ],
      "source": [
        "# Import s3fs for S3 exploration\n",
        "import s3fs\n",
        "\n",
        "# Create S3 filesystem connection\n",
        "s3 = s3fs.S3FileSystem(anon=True)\n",
        "\n",
        "# Explore the NYC TLC bucket structure\n",
        "bucket_path = 's3://nyc-tlc/trip data/'\n",
        "print(f\"Exploring bucket: {bucket_path}\")\n",
        "\n",
        "# List available files (this might take a moment)\n",
        "try:\n",
        "    files = s3.glob(f\"{bucket_path}*.parquet\")\n",
        "    print(f\"Found {len(files)} parquet files\")\n",
        "    \n",
        "    # Show first few files\n",
        "    print(\"\\nSample files:\")\n",
        "    for i, file in enumerate(files[:10]):\n",
        "        print(f\"  {i+1}. {file}\")\n",
        "    \n",
        "    if len(files) > 10:\n",
        "        print(f\"  ... and {len(files) - 10} more files\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error exploring S3 bucket: {e}\")\n",
        "    print(\"This might be due to network connectivity or bucket access restrictions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load NYC Taxi Data\n",
        "\n",
        "Load a sample of NYC taxi data. We'll start with a specific year to keep the dataset manageable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading NYC taxi data for year 2015\n",
            "Data source: s3://nyc-tlc/trip data/yellow_tripdata_2015-*.parquet\n",
            "Error loading data: An error occurred while calling the read_parquet method registered to the pandas backend.\n",
            "Original Message: Access Denied\n",
            "This might be due to network connectivity or data availability\n"
          ]
        }
      ],
      "source": [
        "# Define the data source - using 2015 data as an example\n",
        "# You can change the year or use multiple years\n",
        "year = 2015\n",
        "data_source = f\"s3://nyc-tlc/trip data/yellow_tripdata_{year}-*.parquet\"\n",
        "\n",
        "print(f\"Loading NYC taxi data for year {year}\")\n",
        "print(f\"Data source: {data_source}\")\n",
        "\n",
        "# Load data with Dask (lazy loading)\n",
        "try:\n",
        "    # Read parquet files from S3\n",
        "    df = dd.read_parquet(\n",
        "        data_source,\n",
        "        storage_options={'anon': True}\n",
        "    )\n",
        "    \n",
        "    print(f\"✓ Successfully loaded data\")\n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    print(\"This might be due to network connectivity or data availability\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Explore Data Structure\n",
        "\n",
        "Let's examine the structure and content of the loaded data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Dataset Information ===\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Display basic information about the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Dataset Information ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of partitions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mnpartitions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory usage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mmemory_usage(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"=== Dataset Information ===\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Number of partitions: {df.npartitions}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum().compute() / 1024**3:.2f} GB\")\n",
        "\n",
        "# Show column information\n",
        "print(\"\\n=== Column Information ===\")\n",
        "print(f\"Total columns: {len(df.columns)}\")\n",
        "print(\"Columns:\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "# Show data types\n",
        "print(\"\\n=== Data Types ===\")\n",
        "dtypes = df.dtypes.compute()\n",
        "for col, dtype in dtypes.items():\n",
        "    print(f\"  {col}: {dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"=== Sample Data ===\")\n",
        "sample_data = df.head()\n",
        "print(sample_data)\n",
        "\n",
        "# Display last few rows\n",
        "print(\"\\n=== Last Few Rows ===\")\n",
        "tail_data = df.tail()\n",
        "print(tail_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Analysis\n",
        "\n",
        "Perform some basic analysis on the taxi data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics for numerical columns\n",
        "print(\"=== Basic Statistics ===\")\n",
        "try:\n",
        "    # Get numerical columns\n",
        "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    print(f\"Numerical columns: {list(numerical_cols)}\")\n",
        "    \n",
        "    if len(numerical_cols) > 0:\n",
        "        stats = df[numerical_cols].describe().compute()\n",
        "        print(\"\\nDescriptive Statistics:\")\n",
        "        print(stats)\n",
        "    else:\n",
        "        print(\"No numerical columns found\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error computing statistics: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze specific columns if they exist\n",
        "print(\"=== Column Analysis ===\")\n",
        "\n",
        "# Check for common taxi data columns\n",
        "common_columns = {\n",
        "    'trip_distance': 'Trip distance in miles',\n",
        "    'fare_amount': 'Fare amount in dollars',\n",
        "    'tip_amount': 'Tip amount in dollars',\n",
        "    'total_amount': 'Total amount in dollars',\n",
        "    'passenger_count': 'Number of passengers',\n",
        "    'pickup_datetime': 'Pickup date and time',\n",
        "    'dropoff_datetime': 'Dropoff date and time'\n",
        "}\n",
        "\n",
        "available_columns = []\n",
        "for col, description in common_columns.items():\n",
        "    if col in df.columns:\n",
        "        available_columns.append((col, description))\n",
        "        print(f\"✓ {col}: {description}\")\n",
        "    else:\n",
        "        print(f\"✗ {col}: Not found\")\n",
        "\n",
        "print(f\"\\nFound {len(available_columns)} common taxi columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Visualization\n",
        "\n",
        "Create some visualizations to understand the data better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations for available numerical columns\n",
        "if len(available_columns) > 0:\n",
        "    print(\"Creating visualizations...\")\n",
        "    \n",
        "    # Set up the plotting area\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle(f'NYC Taxi Data Analysis - {year}', fontsize=16)\n",
        "    \n",
        "    plot_count = 0\n",
        "    \n",
        "    # Plot available numerical columns\n",
        "    for col, description in available_columns[:4]:  # Limit to 4 plots\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                row = plot_count // 2\n",
        "                col_idx = plot_count % 2\n",
        "                \n",
        "                # Sample data for plotting (to avoid memory issues)\n",
        "                sample_data = df[col].dropna().sample(frac=0.1).compute()\n",
        "                \n",
        "                if len(sample_data) > 0:\n",
        "                    axes[row, col_idx].hist(sample_data, bins=50, alpha=0.7, edgecolor='black')\n",
        "                    axes[row, col_idx].set_title(f'{col} Distribution')\n",
        "                    axes[row, col_idx].set_xlabel(col)\n",
        "                    axes[row, col_idx].set_ylabel('Frequency')\n",
        "                    \n",
        "                    plot_count += 1\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error plotting {col}: {e}\")\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(plot_count, 4):\n",
        "        row = i // 2\n",
        "        col_idx = i % 2\n",
        "        axes[row, col_idx].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"No suitable columns found for visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "\n",
        "### What we accomplished:\n",
        "1. **Connected to S3** and explored available NYC taxi data\n",
        "2. **Loaded data** using Dask for efficient processing\n",
        "3. **Explored data structure** and content\n",
        "4. **Performed analysis** on trip patterns, distances, and fares\n",
        "5. **Created visualizations** to understand data distributions\n",
        "\n",
        "### Key Features:\n",
        "- **Lazy loading**: Data is loaded on-demand to save memory\n",
        "- **Parallel processing**: Dask processes data across multiple cores\n",
        "- **Scalable**: Can handle datasets much larger than available memory\n",
        "- **Interactive**: Easy to explore and analyze data\n",
        "\n",
        "### Next Steps:\n",
        "- Try different years of data\n",
        "- Combine multiple years for longitudinal analysis\n",
        "- Add more sophisticated analysis (geospatial, temporal patterns)\n",
        "- Export to different formats (CSV, database, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nAnalysis completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"✓ NYC Taxi Data Analysis Complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dask-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
