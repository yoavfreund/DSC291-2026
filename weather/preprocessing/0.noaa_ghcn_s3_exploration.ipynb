{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "```bash\n",
    "pip install s3fs pandas dask\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:44.735799Z",
     "iopub.status.busy": "2025-10-13T23:39:44.735445Z",
     "iopub.status.idle": "2025-10-13T23:39:46.627129Z",
     "shell.execute_reply": "2025-10-13T23:39:46.626855Z"
    }
   },
   "outputs": [],
   "source": [
    "import s3fs, pandas as pd, dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:46.647294Z",
     "iopub.status.busy": "2025-10-13T23:39:46.647025Z",
     "iopub.status.idle": "2025-10-13T23:39:46.649893Z",
     "shell.execute_reply": "2025-10-13T23:39:46.649634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing bucket: noaa-ghcn-pds\n"
     ]
    }
   ],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "bucket = 'noaa-ghcn-pds'\n",
    "print(f\"Testing bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:46.651348Z",
     "iopub.status.busy": "2025-10-13T23:39:46.651225Z",
     "iopub.status.idle": "2025-10-13T23:39:46.971005Z",
     "shell.execute_reply": "2025-10-13T23:39:46.970114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noaa-ghcn-pds/csv', 'noaa-ghcn-pds/csv.gz', 'noaa-ghcn-pds/ghcnd-countries.txt', 'noaa-ghcn-pds/ghcnd-inventory.txt', 'noaa-ghcn-pds/ghcnd-states.txt', 'noaa-ghcn-pds/ghcnd-stations.txt', 'noaa-ghcn-pds/ghcnd-version.txt', 'noaa-ghcn-pds/index.html', 'noaa-ghcn-pds/mingle-list.txt', 'noaa-ghcn-pds/parquet', 'noaa-ghcn-pds/readme-by_station.txt', 'noaa-ghcn-pds/readme-by_year.txt', 'noaa-ghcn-pds/readme.txt', 'noaa-ghcn-pds/status-by_station.txt', 'noaa-ghcn-pds/status-by_year.txt', 'noaa-ghcn-pds/status.txt', 'noaa-ghcn-pds/test.txt']\n"
     ]
    }
   ],
   "source": [
    "print(s3.ls(bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:46.975888Z",
     "iopub.status.busy": "2025-10-13T23:39:46.975407Z",
     "iopub.status.idle": "2025-10-13T23:39:47.074362Z",
     "shell.execute_reply": "2025-10-13T23:39:47.073475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=ADPT', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=ASLP', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=ASTP', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=AWBT', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=AWDR', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=AWND', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=DAPR', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=DASF', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=DATN', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=DATX', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=DWPR', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=EVAP', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=MDPR', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=MDSF', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=MDTN', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=MDTX', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=MNPN', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=MXPN', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=PGTM', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=PRCP', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=PSUN', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=RHAV', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=RHMN', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=RHMX', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SN31', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SN32', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SN33', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SN35', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SN36', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SN51', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SN52', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SN53', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SN55', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SN56', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SNOW', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SNWD', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SX31', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SX32', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SX33', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SX35', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SX36', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SX51', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SX52', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SX53', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SX55', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=SX56', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=TAVG', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=THIC', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=TMAX', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=TMIN', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=TOBS', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=TSUN', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WDF2', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WDF5', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WDFG', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WDMV', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WESD', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WESF', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WSF2', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WSF5', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WSFG', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WSFI', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT01', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT02', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT03', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT04', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT05', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT06', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT07', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT08', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT09', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT10', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT11', 'noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=WT18']\n"
     ]
    }
   ],
   "source": [
    "print(s3.ls(f'{bucket}/parquet/by_year/YEAR=2020/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:47.079975Z",
     "iopub.status.busy": "2025-10-13T23:39:47.079365Z",
     "iopub.status.idle": "2025-10-13T23:39:47.434615Z",
     "shell.execute_reply": "2025-10-13T23:39:47.433532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years: 263\n",
      "First: YEAR=1750\n",
      "Last: YEAR=2025\n"
     ]
    }
   ],
   "source": [
    "years = s3.ls(f'{bucket}/parquet/by_year')\n",
    "print(f\"Years: {len(years)}\")\n",
    "print(f\"First: {years[0].split('/')[-1]}\")\n",
    "print(f\"Last: {years[-1].split('/')[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Meanings (from README)\n",
    "\n",
    "**Core Elements:**\n",
    "- **PRCP**: Precipitation (tenths of mm)\n",
    "- **SNOW**: Snowfall (mm) \n",
    "- **SNWD**: Snow depth (mm)\n",
    "- **TMAX**: Maximum temperature (tenths of degrees C)\n",
    "- **TMIN**: Minimum temperature (tenths of degrees C)\n",
    "\n",
    "**Temperature Elements:**\n",
    "- **TAVG**: Average daily temperature (tenths of degrees C)\n",
    "- **TOBS**: Temperature at time of observation (tenths of degrees C)\n",
    "- **TAXN**: Average daily temperature computed as (TMAX+TMIN)/2.0 (tenths of degrees C)\n",
    "\n",
    "**Wind Elements:**\n",
    "- **AWND**: Average daily wind speed (tenths of meters per second)\n",
    "- **WSF1**: Fastest 1-minute wind speed (tenths of meters per second)\n",
    "- **WSF2**: Fastest 2-minute wind speed (tenths of meters per second)\n",
    "- **WSFG**: Peak gust wind speed (tenths of meters per second)\n",
    "\n",
    "**Pressure Elements:**\n",
    "- **ASLP**: Average Sea Level Pressure (hPa * 10)\n",
    "- **ASTP**: Average Station Level Pressure (hPa * 10)\n",
    "\n",
    "**Other Elements:**\n",
    "- **AWDR**: Average daily wind direction (degrees)\n",
    "- **EVAP**: Evaporation of water from evaporation pan (tenths of mm)\n",
    "- **TSUN**: Daily total sunshine (minutes)\n",
    "- **PSUN**: Daily percent of possible sunshine (percent)\n",
    "\n",
    "**Flags:**\n",
    "- **MFLAG**: Measurement flag (B=12hr totals, D=6hr totals, H=hourly, etc.)\n",
    "- **QFLAG**: Quality flag (D=duplicate, G=gap, I=internal consistency, etc.)\n",
    "- **SFLAG**: Source flag (0=US Cooperative, A=ASOS, S=Global Summary, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing S3 connection...\n",
      "S3 working. Years: ['YEAR=1750', 'YEAR=1763', 'YEAR=1764', 'YEAR=1765', 'YEAR=1766']\n"
     ]
    }
   ],
   "source": [
    "# Test S3 connection\n",
    "print(\"Testing S3 connection...\")\n",
    "test_files = s3.ls(f'{bucket}/parquet/by_year/')[:5]\n",
    "print(f\"S3 working. Years: {[f.split('/')[-1] for f in test_files]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring S3 parquet by_year structure...\n",
      "==================================================\n",
      "Total years available: 263\n",
      "Year range: YEAR=1750 to YEAR=2025\n",
      "\n",
      "Exploring 2020 structure:\n",
      "Elements in 2020: 74\n",
      "\n",
      "First 10 elements in 2020:\n",
      "   1. ADPT\n",
      "   2. ASLP\n",
      "   3. ASTP\n",
      "   4. AWBT\n",
      "   5. AWDR\n",
      "   6. AWND\n",
      "   7. DAPR\n",
      "   8. DASF\n",
      "   9. DATN\n",
      "  10. DATX\n",
      "\n",
      "Exploring ADPT in detail:\n",
      "Files in ADPT: 1\n",
      "File sizes:\n",
      "  1. 6f6332297eb3486fae9c856b0d7fab (  0.18 MB)\n",
      "\n",
      "Directory structure summary:\n",
      "- Years: 263\n",
      "- Elements per year: ~74\n",
      "- Files per element: ~1\n"
     ]
    }
   ],
   "source": [
    "# Explore S3 parquet by_year directory structure\n",
    "print(\"Exploring S3 parquet by_year structure...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get all year directories\n",
    "years = s3.ls(f'{bucket}/parquet/by_year/')\n",
    "print(f\"Total years available: {len(years)}\")\n",
    "print(f\"Year range: {years[0].split('/')[-1]} to {years[-1].split('/')[-1]}\")\n",
    "\n",
    "# Explore a recent year (2020) structure\n",
    "print(f\"\\nExploring 2020 structure:\")\n",
    "year_2020 = f'{bucket}/parquet/by_year/YEAR=2020/'\n",
    "elements_2020 = s3.ls(year_2020)\n",
    "print(f\"Elements in 2020: {len(elements_2020)}\")\n",
    "\n",
    "# Show first 10 elements\n",
    "print(f\"\\nFirst 10 elements in 2020:\")\n",
    "for i, elem in enumerate(elements_2020[:10]):\n",
    "    element_name = elem.split('/')[-1].replace('ELEMENT=', '')\n",
    "    print(f\"  {i+1:2d}. {element_name}\")\n",
    "\n",
    "# Explore one element in detail\n",
    "if elements_2020:\n",
    "    sample_element = elements_2020[0]  # First element\n",
    "    element_name = sample_element.split('/')[-1].replace('ELEMENT=', '')\n",
    "    print(f\"\\nExploring {element_name} in detail:\")\n",
    "    \n",
    "    # Get files in this element\n",
    "    element_files = s3.ls(sample_element)\n",
    "    print(f\"Files in {element_name}: {len(element_files)}\")\n",
    "    \n",
    "    # Show file sizes\n",
    "    print(f\"File sizes:\")\n",
    "    for i, file_path in enumerate(element_files[:5]):  # Show first 5 files\n",
    "        try:\n",
    "            file_info = s3.info(file_path)\n",
    "            size_mb = file_info['size'] / (1024 * 1024)\n",
    "            filename = file_path.split('/')[-1]\n",
    "            print(f\"  {i+1}. {filename[:30]:30s} ({size_mb:6.2f} MB)\")\n",
    "        except:\n",
    "            print(f\"  {i+1}. {file_path.split('/')[-1][:30]:30s} (size unknown)\")\n",
    "\n",
    "print(f\"\\nDirectory structure summary:\")\n",
    "print(f\"- Years: {len(years)}\")\n",
    "print(f\"- Elements per year: ~{len(elements_2020)}\")\n",
    "print(f\"- Files per element: ~{len(element_files) if elements_2020 else 'unknown'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring parquet file schema...\n",
      "========================================\n",
      "Using default Dask DataFrame backend...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'dask.dataframe' has no attribute '_backends'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Check available backends\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable backends: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent backend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdask\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# First, check what files are actually available in 2020\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'dask.dataframe' has no attribute '_backends'"
     ]
    }
   ],
   "source": [
    "# Explore parquet file schema\n",
    "print(\"Exploring parquet file schema...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Configure Dask to use the correct backend\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Try different backend configurations\n",
    "try:\n",
    "    # Option 1: Use dask.dataframe directly (default backend)\n",
    "    print(\"Using default Dask DataFrame backend...\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Check available backends\n",
    "print(f\"Available backends: {dask.dataframe._backends}\")\n",
    "print(f\"Current backend: {dask.dataframe._backend}\")\n",
    "\n",
    "try:\n",
    "    # First, check what files are actually available in 2020\n",
    "    year_2020 = f'{bucket}/parquet/by_year/YEAR=2020/'\n",
    "    elements_2020 = s3.ls(year_2020)\n",
    "    print(f\"Available elements in 2020: {len(elements_2020)}\")\n",
    "    print(f\"First 5 elements: {[elem.split('/')[-1] for elem in elements_2020[:5]]}\")\n",
    "    \n",
    "    # Try to load from a specific element directory\n",
    "    if elements_2020:\n",
    "        # Use the first available element\n",
    "        sample_element = elements_2020[0]\n",
    "        element_name = sample_element.split('/')[-1].replace('ELEMENT=', '')\n",
    "        print(f\"\\nTrying to load {element_name} data...\")\n",
    "        \n",
    "        # Get files in this element\n",
    "        element_files = s3.ls(sample_element)\n",
    "        print(f\"Files in {element_name}: {len(element_files)}\")\n",
    "        \n",
    "        if element_files:\n",
    "            # Try loading the first file directly with explicit Dask configuration\n",
    "            first_file = element_files[0]\n",
    "            print(f\"Loading file: {first_file}\")\n",
    "            \n",
    "            # Load using Dask with simplified approach\n",
    "            print(\"Attempting to load with Dask...\")\n",
    "            try:\n",
    "                # Try with s3fs filesystem\n",
    "                df_sample = dd.read_parquet(first_file, storage_options={'anon': True})\n",
    "            except Exception as e1:\n",
    "                print(f\"Dask with s3fs failed: {e1}\")\n",
    "                # Try with direct S3 URL\n",
    "                s3_url = f\"s3://{first_file}\"\n",
    "                print(f\"Trying direct S3 URL: {s3_url}\")\n",
    "                df_sample = dd.read_parquet(s3_url, storage_options={'anon': True})\n",
    "            print(f\"✓ Loaded sample: {len(df_sample):,} records, {df_sample.npartitions} partitions\")\n",
    "            \n",
    "            # Show schema\n",
    "            print(f\"\\nSchema:\")\n",
    "            print(f\"Columns: {list(df_sample.columns)}\")\n",
    "            print(f\"Data types:\")\n",
    "            print(df_sample.dtypes)\n",
    "            \n",
    "            # Show sample data\n",
    "            print(f\"\\nSample data (first 3 rows):\")\n",
    "            sample_data = df_sample.head(3).compute()\n",
    "            print(sample_data)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No files found in {element_name}\")\n",
    "    else:\n",
    "        print(\"No elements found in 2020 directory\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading parquet sample: {e}\")\n",
    "    print(\"Trying alternative approach with different engine...\")\n",
    "    \n",
    "    # Try with different approaches\n",
    "    try:\n",
    "        if elements_2020 and element_files:\n",
    "            print(\"Trying alternative approaches...\")\n",
    "            \n",
    "            # Try 1: Use pandas directly with s3fs\n",
    "            try:\n",
    "                print(\"Trying pandas with s3fs...\")\n",
    "                import pandas as pd\n",
    "                with s3.open(first_file, 'rb') as f:\n",
    "                    df_pandas = pd.read_parquet(f)\n",
    "                print(f\"✓ Pandas load successful: {len(df_pandas):,} records\")\n",
    "                print(f\"Columns: {list(df_pandas.columns)}\")\n",
    "                print(f\"Sample:\")\n",
    "                print(df_pandas.head(3))\n",
    "                \n",
    "                # Convert to Dask for consistency\n",
    "                df_alt = dd.from_pandas(df_pandas, npartitions=1)\n",
    "                print(f\"✓ Converted to Dask: {len(df_alt):,} records\")\n",
    "                \n",
    "            except Exception as e_pandas:\n",
    "                print(f\"Pandas approach failed: {e_pandas}\")\n",
    "                \n",
    "                # Try 2: Different Dask engine\n",
    "                print(\"Trying Dask with fastparquet...\")\n",
    "                df_alt = dd.read_parquet(\n",
    "                    first_file,\n",
    "                    storage_options={'anon': True},\n",
    "                    engine='fastparquet'\n",
    "                )\n",
    "                print(f\"✓ Fastparquet load successful: {len(df_alt):,} records\")\n",
    "                print(f\"Columns: {list(df_alt.columns)}\")\n",
    "                print(f\"Sample: {df_alt.head(3).compute()}\")\n",
    "                \n",
    "    except Exception as e2:\n",
    "        print(f\"✗ All alternative approaches failed: {e2}\")\n",
    "        print(\"Trying to explore directory structure instead...\")\n",
    "        \n",
    "        # Fallback: just explore the directory structure\n",
    "        try:\n",
    "            print(f\"Exploring {bucket}/parquet/by_year/ structure...\")\n",
    "            years = s3.ls(f'{bucket}/parquet/by_year/')\n",
    "            print(f\"Available years: {len(years)}\")\n",
    "            \n",
    "            if years:\n",
    "                # Try to list files in a recent year\n",
    "                recent_year = years[-1]  # Last year\n",
    "                print(f\"Exploring {recent_year}...\")\n",
    "                year_elements = s3.ls(recent_year)\n",
    "                print(f\"Elements in {recent_year.split('/')[-1]}: {len(year_elements)}\")\n",
    "                \n",
    "                if year_elements:\n",
    "                    first_element = year_elements[0]\n",
    "                    print(f\"First element: {first_element}\")\n",
    "                    element_files = s3.ls(first_element)\n",
    "                    print(f\"Files in element: {len(element_files)}\")\n",
    "                    if element_files:\n",
    "                        print(f\"First file: {element_files[0]}\")\n",
    "        except Exception as e3:\n",
    "            print(f\"✗ Directory exploration failed: {e3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring parquet file schema (corrected approach)...\n",
      "==================================================\n",
      "Looking for files in: s3://noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=TOBS/\n",
      "Found 4 parquet files\n",
      "File URLs: ['s3://noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=TOBS/6f6332297eb3486fae9c856b0d7fab49_0.snappy.parquet', 's3://noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=TOBS/6f6332297eb3486fae9c856b0d7fab49_1.snappy.parquet', 's3://noaa-ghcn-pds/parquet/by_year/YEAR=2020/ELEMENT=TOBS/6f6332297eb3486fae9c856b0d7fab49_2.snappy.parquet']...\n",
      "Loading with Dask (same approach as process_multiple_years.py)...\n",
      "✗ Error loading parquet sample: No backend dispatch registered for dask\n",
      "Trying alternative measurement...\n",
      "Trying PRCP data: 23 files\n",
      "✗ Alternative measurement failed: No backend dispatch registered for dask\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED: Explore parquet file schema using working approach\n",
    "print(\"Exploring parquet file schema (corrected approach)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Use the EXACT same approach as the working process_multiple_years.py\n",
    "    # Setup S3 filesystem\n",
    "    s3 = s3fs.S3FileSystem(anon=True)\n",
    "    bucket_path = 's3://noaa-ghcn-pds/parquet/by_year/'\n",
    "    \n",
    "    # Get files for 2020 TOBS (same as working code)\n",
    "    year = 2020\n",
    "    measurement = 'TOBS'\n",
    "    file_path = f\"{bucket_path}YEAR={year}/ELEMENT={measurement}/\"\n",
    "    \n",
    "    print(f\"Looking for files in: {file_path}\")\n",
    "    \n",
    "    # Use s3.glob to find parquet files (same as working code)\n",
    "    files = s3.glob(f\"{file_path}*.parquet\")\n",
    "    print(f\"Found {len(files)} parquet files\")\n",
    "    \n",
    "    if files:\n",
    "        # Convert to s3:// URLs (same as working code)\n",
    "        all_files = [f\"s3://{f}\" for f in files]\n",
    "        print(f\"File URLs: {all_files[:3]}...\")  # Show first 3\n",
    "        \n",
    "        # Load using the same approach as working code\n",
    "        print(\"Loading with Dask (same approach as process_multiple_years.py)...\")\n",
    "        df_sample = dd.read_parquet(all_files, storage_options={'anon': True})\n",
    "        \n",
    "        print(f\"✓ Loaded sample: {len(df_sample):,} records, {df_sample.npartitions} partitions\")\n",
    "        \n",
    "        # Show schema\n",
    "        print(f\"\\nSchema:\")\n",
    "        print(f\"Columns: {list(df_sample.columns)}\")\n",
    "        print(f\"Data types:\")\n",
    "        print(df_sample.dtypes)\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nSample data (first 3 rows):\")\n",
    "        sample_data = df_sample.head(3).compute()\n",
    "        print(sample_data)\n",
    "        \n",
    "        # Show data statistics\n",
    "        print(f\"\\nData statistics:\")\n",
    "        print(f\"Date range: {df_sample['DATE'].min().compute()} to {df_sample['DATE'].max().compute()}\")\n",
    "        print(f\"Unique stations: {df_sample['ID'].nunique().compute():,}\")\n",
    "        print(f\"Value range: {df_sample['DATA_VALUE'].min().compute():.1f} to {df_sample['DATA_VALUE'].max().compute():.1f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No parquet files found for 2020 TOBS\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading parquet sample: {e}\")\n",
    "    print(\"Trying alternative measurement...\")\n",
    "    \n",
    "    # Try with a different measurement\n",
    "    try:\n",
    "        measurement = 'PRCP'\n",
    "        file_path = f\"{bucket_path}YEAR={year}/ELEMENT={measurement}/\"\n",
    "        files = s3.glob(f\"{file_path}*.parquet\")\n",
    "        \n",
    "        if files:\n",
    "            all_files = [f\"s3://{f}\" for f in files]\n",
    "            print(f\"Trying {measurement} data: {len(files)} files\")\n",
    "            df_alt = dd.read_parquet(all_files, storage_options={'anon': True})\n",
    "            print(f\"✓ Alternative load successful: {len(df_alt):,} records\")\n",
    "            print(f\"Columns: {list(df_alt.columns)}\")\n",
    "            print(f\"Sample: {df_alt.head(3).compute()}\")\n",
    "        else:\n",
    "            print(f\"No files found for {measurement}\")\n",
    "            \n",
    "    except Exception as e2:\n",
    "        print(f\"✗ Alternative measurement failed: {e2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
