{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Concatenate Weather Data (1950-1954)\n",
        "\n",
        "Simple notebook to concatenate weather parquet files for years 1950-1954.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting at 2025-10-17 14:50:27\n"
          ]
        }
      ],
      "source": [
        "import dask.dataframe as dd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"Starting at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Found: 1950\n",
            "✓ Found: 1951\n",
            "✓ Found: 1952\n",
            "✓ Found: 1953\n",
            "✓ Found: 1954\n",
            "\n",
            "Found 5 files\n"
          ]
        }
      ],
      "source": [
        "# Define years and collect file paths\n",
        "years = [1950, 1951, 1952, 1953, 1954]\n",
        "data_dir = \"data\"\n",
        "\n",
        "file_paths = []\n",
        "for year in years:\n",
        "    file_path = os.path.join(data_dir, f\"weather_{year}_wide.parquet\")\n",
        "    if os.path.exists(file_path):\n",
        "        file_paths.append(file_path)\n",
        "        print(f\"✓ Found: {year}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {year}\")\n",
        "\n",
        "print(f\"\\nFound {len(file_paths)} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and concatenating files...\n",
            "  Loaded: data/weather_1950_wide.parquet\n",
            "  Loaded: data/weather_1951_wide.parquet\n",
            "  Loaded: data/weather_1952_wide.parquet\n",
            "  Loaded: data/weather_1953_wide.parquet\n",
            "  Loaded: data/weather_1954_wide.parquet\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported format string passed to Delayed.__format__",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Concatenate all dataframes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mconcat(dataframes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConcatenated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_combined\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows × \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_combined\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Delayed.__format__"
          ]
        }
      ],
      "source": [
        "# Load and concatenate all files\n",
        "print(\"Loading and concatenating files...\")\n",
        "\n",
        "dataframes = []\n",
        "for file_path in file_paths:\n",
        "    df = dd.read_parquet(file_path)\n",
        "    dataframes.append(df)\n",
        "    print(f\"  Loaded: {file_path}\")\n",
        "\n",
        "# Concatenate all dataframes\n",
        "df_combined = dd.concat(dataframes, ignore_index=True)\n",
        "print(f\"\\nConcatenated: {df_combined.shape[0]:,} rows × {len(df_combined.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reference column structure (368 columns):\n",
            "  ['ID', 'year', 'ELEMENT', 'day_1', 'day_2', 'day_3', 'day_4', 'day_5', 'day_6', 'day_7']...\n",
            "✓ Dataframe 2 columns match reference\n",
            "✓ Dataframe 3 columns match reference\n",
            "✓ Dataframe 4 columns match reference\n",
            "✓ Dataframe 5 columns match reference\n",
            "\n",
            "✅ All dataframes have consistent column structure\n"
          ]
        }
      ],
      "source": [
        "# Save the combined data\n",
        "output_file = os.path.join(data_dir, \"weather_1950_1954_combined.parquet\")\n",
        "print(f\"Saving to: {output_file}\")\n",
        "\n",
        "df_combined.to_parquet(output_file)\n",
        "print(\"✓ Saved successfully\")\n",
        "\n",
        "# Check file size\n",
        "if os.path.exists(output_file):\n",
        "    file_size = os.path.getsize(output_file) / (1024 * 1024)\n",
        "    print(f\"File size: {file_size:.1f} MB\")\n",
        "\n",
        "print(f\"\\nCompleted at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenating 5 dataframes...\n",
            "Combined dataframe: 365,437 rows × 368 columns\n",
            "Columns: ['ID', 'year', 'ELEMENT', 'day_1', 'day_2', 'day_3', 'day_4', 'day_5', 'day_6', 'day_7']...\n",
            "Concatenation completed in 2.0 seconds\n"
          ]
        }
      ],
      "source": [
        "# Quick verification\n",
        "print(\"Verifying saved file...\")\n",
        "verification_df = dd.read_parquet(output_file)\n",
        "print(f\"✓ Verification: {verification_df.shape[0]:,} rows × {len(verification_df.columns)} columns\")\n",
        "print(\"✓ All done!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dask-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
