{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load TOBS 2025 Data with Dask\n",
        "\n",
        "This notebook demonstrates how to load and work with the TOBS 2025 wide-format parquet file using Dask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Libraries imported\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print('âœ“ Libraries imported')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 1: Basic Loading with Dask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /home/yfreund/dask-CSE255/weather/tobs_2025_wide.parquet",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/backends.py:125\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py:530\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, chunksize, aggregate_files, parquet_file_extension, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     index \u001b[38;5;241m=\u001b[39m [index]\n\u001b[0;32m--> 530\u001b[0m read_metadata_result \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalculate_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# compatibility with a user-defined engine.\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:423\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, use_nullable_dtypes, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_metadata\u001b[39m(\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_dataset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# Stage 2: Generate output `meta`\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:938\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, chunksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mpa_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_wrapped_fs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_dataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Deal with directory partitioning\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# Get all partition keys (without filters) to populate partition_obj\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/pyarrow/dataset.py:793\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(elem, Dataset) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
            "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/pyarrow/dataset.py:470\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 470\u001b[0m         fs, paths_or_selector \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_multiple_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/pyarrow/dataset.py:379\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[0;34m(paths, filesystem)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mNotFound:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(info\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mDirectory:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: /home/yfreund/dask-CSE255/weather/tobs_2025_wide.parquet",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the parquet file with Dask\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_dask \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtobs_2025_wide.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Dask operations are lazy - this doesn't load the data yet\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDask DataFrame Info:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/.conda/envs/dask-tutorial/lib/python3.10/site-packages/dask/backends.py:127\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /home/yfreund/dask-CSE255/weather/tobs_2025_wide.parquet"
          ]
        }
      ],
      "source": [
        "# Read the parquet file with Dask\n",
        "df_dask = dd.read_parquet('tobs_2025_wide.parquet')\n",
        "\n",
        "# Dask operations are lazy - this doesn't load the data yet\n",
        "print('Dask DataFrame Info:')\n",
        "print(f'  Type: {type(df_dask)}')\n",
        "print(f'  Columns: {len(df_dask.columns)} ({list(df_dask.columns[:5])} ... {list(df_dask.columns[-3:])})')\n",
        "print(f'  Number of partitions: {df_dask.npartitions}')\n",
        "print(f'  Partitions: {df_dask.to_delayed()}')\n",
        "\n",
        "# To see the structure without computing\n",
        "print(f'\\nDataframe structure (lazy):')\n",
        "print(df_dask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing Data (Triggers Computation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows, first 10 columns:\n",
            "    station_id  year  day_1  day_2  day_3  day_4  day_5  day_6  day_7  day_8\n",
            "0  CQC00914801  2025  250.0  244.0  250.0  250.0  256.0  250.0  256.0  244.0\n",
            "1  FMC00914325  2025    NaN  294.0  283.0  283.0  256.0  294.0  294.0  272.0\n",
            "2  FMC00914395  2025  300.0  278.0  306.0  294.0  283.0  311.0  300.0  306.0\n",
            "3  FMC00914590  2025  211.0  250.0  300.0  228.0  289.0  294.0  278.0  239.0\n",
            "4  FMC00914720  2025    NaN  283.0  289.0  278.0  289.0  289.0  283.0  289.0\n",
            "\n",
            "\n",
            "Basic statistics:\n",
            "Number of rows (computed): 4706\n",
            "Unique stations: 4706\n"
          ]
        }
      ],
      "source": [
        "# View first few rows (only loads necessary partitions)\n",
        "print('First 5 rows, first 10 columns:')\n",
        "first_10_cols = list(df_dask.columns[:10])\n",
        "print(df_dask[first_10_cols].head())\n",
        "\n",
        "print('\\n\\nBasic statistics:')\n",
        "print(f'Number of rows (computed): {len(df_dask)}')\n",
        "print(f'Unique stations: {df_dask[\"station_id\"].nunique().compute()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dask Operations (Lazy Evaluation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of day columns: 365\n",
            "\n",
            "Mean computation (lazy): Dask Series Structure:\n",
            "npartitions=1\n",
            "day_1    float64\n",
            "day_9        ...\n",
            "dtype: float64\n",
            "Dask Name: dataframe-mean, 7 graph layers\n",
            "\n",
            "Mean temperatures for first 10 days (in tenths of Â°C):\n",
            "  day_1: -11.4 (= -1.1Â°C)\n",
            "  day_2: -16.6 (= -1.7Â°C)\n",
            "  day_3: -22.9 (= -2.3Â°C)\n",
            "  day_4: -42.4 (= -4.2Â°C)\n",
            "  day_5: -40.4 (= -4.0Â°C)\n",
            "  day_6: -50.5 (= -5.1Â°C)\n",
            "  day_7: -62.8 (= -6.3Â°C)\n",
            "  day_8: -62.0 (= -6.2Â°C)\n",
            "  day_9: -59.0 (= -5.9Â°C)\n",
            "  day_10: -37.5 (= -3.8Â°C)\n"
          ]
        }
      ],
      "source": [
        "# Example: Calculate mean temperature for each day across all stations\n",
        "# This is lazy - builds the computation graph but doesn't execute yet\n",
        "\n",
        "day_cols = [col for col in df_dask.columns if col.startswith('day_')]\n",
        "print(f'Number of day columns: {len(day_cols)}')\n",
        "\n",
        "# Calculate mean for first 10 days (lazy)\n",
        "means_lazy = df_dask[day_cols[:10]].mean()\n",
        "print(f'\\nMean computation (lazy): {means_lazy}')\n",
        "\n",
        "# Now compute the result\n",
        "means = means_lazy.compute()\n",
        "print(f'\\nMean temperatures for first 10 days (in tenths of Â°C):')\n",
        "for col, val in means.items():\n",
        "    print(f'  {col}: {val:.1f} (= {val/10:.1f}Â°C)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with Specific Stations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total stations: 4706\n",
            "First 10 stations: ['CQC00914801', 'FMC00914325', 'FMC00914395', 'FMC00914590', 'FMC00914720', 'GQW00041406', 'RQC00660053', 'RQC00660061', 'RQC00660152', 'RQC00660158']\n",
            "\n",
            "Data for station CQC00914801:\n",
            "    station_id  year  day_1  day_2  day_3  day_4  day_5  day_6  day_7  day_8\n",
            "0  CQC00914801  2025  250.0  244.0  250.0  250.0  256.0  250.0  256.0  244.0\n"
          ]
        }
      ],
      "source": [
        "# Filter for specific stations\n",
        "# Get all station IDs first\n",
        "all_stations = df_dask['station_id'].compute()\n",
        "print(f'Total stations: {len(all_stations)}')\n",
        "print(f'First 10 stations: {all_stations.head(10).tolist()}')\n",
        "\n",
        "# Filter for a specific station (lazy)\n",
        "first_station = all_stations.iloc[0]\n",
        "station_filter = df_dask['station_id'] == first_station\n",
        "single_station = df_dask[station_filter]\n",
        "\n",
        "# Compute to get the data\n",
        "station_data = single_station.compute()\n",
        "print(f'\\nData for station {first_station}:')\n",
        "# Now we can use iloc on the pandas dataframe\n",
        "print(station_data.iloc[:, :10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting to Pandas (if data fits in memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted to Pandas DataFrame:\n",
            "  Type: <class 'pandas.core.frame.DataFrame'>\n",
            "  Shape: (4706, 367)\n",
            "  Memory usage: 13.45 MB\n",
            "\n",
            "First 3 rows, first 8 columns:\n",
            "    station_id  year  day_1  day_2  day_3  day_4  day_5  day_6\n",
            "0  CQC00914801  2025  250.0  244.0  250.0  250.0  256.0  250.0\n",
            "1  FMC00914325  2025    NaN  294.0  283.0  283.0  256.0  294.0\n",
            "2  FMC00914395  2025  300.0  278.0  306.0  294.0  283.0  311.0\n"
          ]
        }
      ],
      "source": [
        "# Since this dataset is relatively small (~1.7 MB), we can load it all into pandas\n",
        "df_pandas = df_dask.compute()\n",
        "\n",
        "print(f'Converted to Pandas DataFrame:')\n",
        "print(f'  Type: {type(df_pandas)}')\n",
        "print(f'  Shape: {df_pandas.shape}')\n",
        "print(f'  Memory usage: {df_pandas.memory_usage(deep=True).sum() / (1024**2):.2f} MB')\n",
        "\n",
        "print(f'\\nFirst 3 rows, first 8 columns:')\n",
        "# Now we can use iloc since it's pandas\n",
        "print(df_pandas.iloc[:3, :8])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Differences: Dask vs Pandas\n",
        "\n",
        "| Operation | Pandas | Dask |\n",
        "|-----------|--------|------|\n",
        "| **Reading** | `pd.read_parquet()` | `dd.read_parquet()` |\n",
        "| **Execution** | Immediate (eager) | Lazy (builds computation graph) |\n",
        "| **Compute** | Automatic | Call `.compute()` to execute |\n",
        "| **Memory** | Loads entire dataset | Can work with data larger than memory |\n",
        "| **Partitions** | Single dataframe | Multiple partitions |\n",
        "\n",
        "### Common Dask Operations\n",
        "\n",
        "```python\n",
        "# Reading\n",
        "df = dd.read_parquet('file.parquet')\n",
        "\n",
        "# Viewing (triggers computation)\n",
        "df.head()           # First few rows\n",
        "df.tail()           # Last few rows\n",
        "df.compute()        # Convert to pandas (full dataset)\n",
        "\n",
        "# Lazy operations (no computation yet)\n",
        "df.mean()           # Calculate means\n",
        "df[df['col'] > 10]  # Filter data\n",
        "df.groupby('col').mean()  # Group and aggregate\n",
        "\n",
        "# Execute computation\n",
        "result = df.mean().compute()  # Now it actually runs\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting TOBS Data for Multiple Stations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "print('âœ“ Plotting libraries imported')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a few stations with good data coverage\n",
        "# First, let's find stations with the most complete data\n",
        "day_cols = [col for col in df_pandas.columns if col.startswith('day_')]\n",
        "df_pandas['completeness'] = df_pandas[day_cols].notna().sum(axis=1)\n",
        "\n",
        "# Get top 5 stations with most complete data\n",
        "top_stations = df_pandas.nlargest(5, 'completeness')\n",
        "print('Top 5 stations by data completeness:')\n",
        "print(top_stations[['station_id', 'completeness']])\n",
        "print(f'\\nPlotting data for these stations...')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot TOBS for the top 5 stations\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "days = list(range(1, 366))  # Days 1-365\n",
        "\n",
        "for idx, row in top_stations.iterrows():\n",
        "    station_id = row['station_id']\n",
        "    # Extract temperature values for all days (convert from tenths to degrees C)\n",
        "    temps = [row[f'day_{d}'] / 10 if pd.notna(row[f'day_{d}']) else np.nan for d in days]\n",
        "    \n",
        "    # Plot with gaps where data is missing\n",
        "    ax.plot(days, temps, marker='o', markersize=2, linewidth=1.5, label=station_id, alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Day of Year (2025)', fontsize=12)\n",
        "ax.set_ylabel('Temperature (Â°C)', fontsize=12)\n",
        "ax.set_title('TOBS (Temperature at Observation Time) for Top 5 Stations in 2025', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(1, 365)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('tobs_top5_stations.png', dpi=150, bbox_inches='tight')\n",
        "print('âœ“ Plot saved as tobs_top5_stations.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot for Specific Regions\n",
        "\n",
        "Let's also look at stations from different geographic regions to see variation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select stations from different regions based on station ID prefixes\n",
        "# US stations start with 'US', Canada with 'CA', etc.\n",
        "regions = {\n",
        "    'US': df_pandas[df_pandas['station_id'].str.startswith('US')],\n",
        "    'CA': df_pandas[df_pandas['station_id'].str.startswith('CA')],\n",
        "    'GM': df_pandas[df_pandas['station_id'].str.startswith('GM')],  # Germany\n",
        "    'JA': df_pandas[df_pandas['station_id'].str.startswith('JA')],  # Japan\n",
        "    'AS': df_pandas[df_pandas['station_id'].str.startswith('AS')],  # Australia\n",
        "}\n",
        "\n",
        "print('Stations by region:')\n",
        "selected_stations = []\n",
        "for region, stations_df in regions.items():\n",
        "    if len(stations_df) > 0:\n",
        "        # Get the station with best coverage from this region\n",
        "        best = stations_df.nlargest(1, 'completeness')\n",
        "        if len(best) > 0:\n",
        "            station_id = best.iloc[0]['station_id']\n",
        "            completeness = best.iloc[0]['completeness']\n",
        "            selected_stations.append(station_id)\n",
        "            print(f'  {region}: {station_id} ({completeness:.0f} days)')\n",
        "\n",
        "print(f'\\nTotal selected: {len(selected_stations)} stations')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot TOBS for stations from different regions\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "days = list(range(1, 366))\n",
        "colors = plt.cm.tab10(range(len(selected_stations)))\n",
        "\n",
        "for i, station_id in enumerate(selected_stations):\n",
        "    station_row = df_pandas[df_pandas['station_id'] == station_id].iloc[0]\n",
        "    \n",
        "    # Extract temperature values (convert from tenths to degrees C)\n",
        "    temps = [station_row[f'day_{d}'] / 10 if pd.notna(station_row[f'day_{d}']) else np.nan for d in days]\n",
        "    \n",
        "    ax.plot(days, temps, marker='o', markersize=2, linewidth=1.5, \n",
        "            label=f'{station_id}', alpha=0.8, color=colors[i])\n",
        "\n",
        "ax.set_xlabel('Day of Year (2025)', fontsize=12)\n",
        "ax.set_ylabel('Temperature (Â°C)', fontsize=12)\n",
        "ax.set_title('TOBS Comparison: Stations from Different Regions', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(1, 365)\n",
        "\n",
        "# Add month labels\n",
        "month_starts = [1, 32, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
        "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "ax.set_xticks(month_starts)\n",
        "ax.set_xticklabels(month_names)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('tobs_regional_comparison.png', dpi=150, bbox_inches='tight')\n",
        "print('âœ“ Plot saved as tobs_regional_comparison.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Individual Station Plots\n",
        "\n",
        "Create separate subplots for better visibility of individual patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a grid of subplots\n",
        "n_stations = min(6, len(selected_stations))\n",
        "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "days = list(range(1, 366))\n",
        "month_starts = [1, 32, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
        "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "\n",
        "for i in range(n_stations):\n",
        "    if i < len(selected_stations):\n",
        "        station_id = selected_stations[i]\n",
        "        station_row = df_pandas[df_pandas['station_id'] == station_id].iloc[0]\n",
        "        \n",
        "        # Extract temperature values (use np.nan instead of None)\n",
        "        temps = np.array([station_row[f'day_{d}'] / 10 if pd.notna(station_row[f'day_{d}']) else np.nan for d in days])\n",
        "        \n",
        "        # Calculate completeness\n",
        "        valid_temps = temps[~np.isnan(temps)]\n",
        "        completeness = len(valid_temps) / 365 * 100\n",
        "        \n",
        "        # Plot\n",
        "        axes[i].plot(days, temps, color='steelblue', linewidth=1.5, alpha=0.8)\n",
        "        axes[i].fill_between(days, temps, alpha=0.3, color='steelblue')\n",
        "        axes[i].set_title(f'{station_id}\\n({len(valid_temps)}/365 days, {completeness:.1f}% complete)', \n",
        "                         fontsize=11, fontweight='bold')\n",
        "        axes[i].set_xlabel('Day of Year')\n",
        "        axes[i].set_ylabel('Temperature (Â°C)')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "        axes[i].set_xlim(1, 365)\n",
        "        axes[i].set_xticks(month_starts)\n",
        "        axes[i].set_xticklabels(month_names, rotation=45)\n",
        "    else:\n",
        "        # Hide unused subplots\n",
        "        axes[i].set_visible(False)\n",
        "\n",
        "plt.suptitle('TOBS by Station - 2025', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('tobs_individual_stations.png', dpi=150, bbox_inches='tight')\n",
        "print('âœ“ Plot saved as tobs_individual_stations.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary Statistics\n",
        "\n",
        "Show overall temperature patterns across all stations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate daily statistics across all stations\n",
        "day_cols = [f'day_{d}' for d in range(1, 366)]\n",
        "daily_stats = df_pandas[day_cols].describe().T\n",
        "\n",
        "# Convert to degrees C\n",
        "daily_stats_celsius = daily_stats / 10\n",
        "\n",
        "# Extract statistics\n",
        "days = list(range(1, 366))\n",
        "mean_temps = daily_stats_celsius['mean'].values\n",
        "min_temps = daily_stats_celsius['min'].values\n",
        "max_temps = daily_stats_celsius['max'].values\n",
        "q25_temps = daily_stats_celsius['25%'].values\n",
        "q75_temps = daily_stats_celsius['75%'].values\n",
        "\n",
        "# Create plot\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Plot ranges and means\n",
        "ax.fill_between(days, min_temps, max_temps, alpha=0.15, color='gray', label='Min-Max Range')\n",
        "ax.fill_between(days, q25_temps, q75_temps, alpha=0.3, color='steelblue', label='25th-75th Percentile')\n",
        "ax.plot(days, mean_temps, color='darkred', linewidth=2.5, label='Mean', alpha=0.9)\n",
        "\n",
        "ax.set_xlabel('Day of Year (2025)', fontsize=12)\n",
        "ax.set_ylabel('Temperature (Â°C)', fontsize=12)\n",
        "ax.set_title('Daily Temperature Statistics Across All Stations - 2025', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='best', fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(1, 365)\n",
        "\n",
        "# Add month labels\n",
        "month_starts = [1, 32, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
        "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "ax.set_xticks(month_starts)\n",
        "ax.set_xticklabels(month_names)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('tobs_daily_statistics.png', dpi=150, bbox_inches='tight')\n",
        "print('âœ“ Plot saved as tobs_daily_statistics.png')\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nðŸ“Š Overall Statistics:')\n",
        "print(f'   Mean temperature: {mean_temps[~np.isnan(mean_temps)].mean():.1f}Â°C')\n",
        "print(f'   Overall min: {np.nanmin(min_temps):.1f}Â°C')\n",
        "print(f'   Overall max: {np.nanmax(max_temps):.1f}Â°C')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dask-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
