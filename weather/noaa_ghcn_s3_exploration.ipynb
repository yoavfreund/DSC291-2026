{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install required packages:\n",
    "```bash\n",
    "pip install s3fs boto3 pandas pyarrow dask matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:44.735799Z",
     "iopub.status.busy": "2025-10-13T23:39:44.735445Z",
     "iopub.status.idle": "2025-10-13T23:39:46.627129Z",
     "shell.execute_reply": "2025-10-13T23:39:46.626855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print('✓ Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to S3 Bucket\n",
    "\n",
    "The NOAA GHCN data is publicly available (no credentials needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:46.647294Z",
     "iopub.status.busy": "2025-10-13T23:39:46.647025Z",
     "iopub.status.idle": "2025-10-13T23:39:46.649893Z",
     "shell.execute_reply": "2025-10-13T23:39:46.649634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to S3...\n",
      "✓ Connected to S3 bucket: noaa-ghcn-pds\n"
     ]
    }
   ],
   "source": [
    "# Initialize S3 filesystem (anonymous access)\n",
    "print('Connecting to S3...')\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# NOAA GHCN bucket\n",
    "bucket_name = 'noaa-ghcn-pds'\n",
    "print(f'✓ Connected to S3 bucket: {bucket_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Bucket Structure\n",
    "\n",
    "Let's see how the data is organized in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:46.651348Z",
     "iopub.status.busy": "2025-10-13T23:39:46.651225Z",
     "iopub.status.idle": "2025-10-13T23:39:46.971005Z",
     "shell.execute_reply": "2025-10-13T23:39:46.970114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level directories in the bucket:\n",
      "============================================================\n",
      "  noaa-ghcn-pds/csv\n",
      "  noaa-ghcn-pds/csv.gz\n",
      "  noaa-ghcn-pds/ghcnd-countries.txt\n",
      "  noaa-ghcn-pds/ghcnd-inventory.txt\n",
      "  noaa-ghcn-pds/ghcnd-states.txt\n",
      "  noaa-ghcn-pds/ghcnd-stations.txt\n",
      "  noaa-ghcn-pds/ghcnd-version.txt\n",
      "  noaa-ghcn-pds/index.html\n",
      "  noaa-ghcn-pds/mingle-list.txt\n",
      "  noaa-ghcn-pds/parquet\n",
      "  noaa-ghcn-pds/readme-by_station.txt\n",
      "  noaa-ghcn-pds/readme-by_year.txt\n",
      "  noaa-ghcn-pds/readme.txt\n",
      "  noaa-ghcn-pds/status-by_station.txt\n",
      "  noaa-ghcn-pds/status-by_year.txt\n",
      "  noaa-ghcn-pds/status.txt\n",
      "  noaa-ghcn-pds/test.txt\n",
      "\n",
      "Total items: 17\n"
     ]
    }
   ],
   "source": [
    "# List top-level directories\n",
    "print('Top-level directories in the bucket:')\n",
    "print('='*60)\n",
    "top_level = s3.ls(bucket_name)\n",
    "for item in top_level:\n",
    "    print(f'  {item}')\n",
    "\n",
    "print(f'\\nTotal items: {len(top_level)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:46.975888Z",
     "iopub.status.busy": "2025-10-13T23:39:46.975407Z",
     "iopub.status.idle": "2025-10-13T23:39:47.074362Z",
     "shell.execute_reply": "2025-10-13T23:39:47.073475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring parquet data organization:\n",
      "============================================================\n",
      "  noaa-ghcn-pds/parquet/by_station\n",
      "  noaa-ghcn-pds/parquet/by_year\n"
     ]
    }
   ],
   "source": [
    "# Explore parquet directory\n",
    "print('Exploring parquet data organization:')\n",
    "print('='*60)\n",
    "parquet_dirs = s3.ls(f'{bucket_name}/parquet/')\n",
    "for item in parquet_dirs:\n",
    "    print(f'  {item}')\n",
    "    \n",
    "    # Get size info if it's a directory\n",
    "    if item.endswith('/'):\n",
    "        subfiles = s3.ls(item)\n",
    "        print(f'    └─ Contains {len(subfiles)} items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore by_year Parquet Files\n",
    "\n",
    "Let's investigate the file organization and schema of the by_year parquet data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:47.079975Z",
     "iopub.status.busy": "2025-10-13T23:39:47.079365Z",
     "iopub.status.idle": "2025-10-13T23:39:47.434615Z",
     "shell.execute_reply": "2025-10-13T23:39:47.433532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BY_YEAR FILE ORGANIZATION\n",
      "================================================================================\n",
      "\n",
      "Total year directories: 264\n",
      "\n",
      "First 10 years:\n",
      "   1. YEAR=1750\n",
      "   2. YEAR=1763\n",
      "   3. YEAR=1764\n",
      "   4. YEAR=1765\n",
      "   5. YEAR=1766\n",
      "   6. YEAR=1767\n",
      "   7. YEAR=1768\n",
      "   8. YEAR=1769\n",
      "   9. YEAR=1770\n",
      "  10. YEAR=1771\n",
      "\n",
      "Last 10 years:\n",
      "   1. YEAR=2016\n",
      "   2. YEAR=2017\n",
      "   3. YEAR=2018\n",
      "   4. YEAR=2019\n",
      "   5. YEAR=2020\n",
      "   6. YEAR=2021\n",
      "   7. YEAR=2022\n",
      "   8. YEAR=2023\n",
      "   9. YEAR=2024\n",
      "  10. YEAR=2025\n",
      "\n",
      "\n",
      "Exploring elements in YEAR=2012:\n",
      "--------------------------------------------------------------------------------\n",
      "Total elements: 98\n",
      "\n",
      "First 20 elements:\n",
      "   1. ELEMENT=ADPT\n",
      "   2. ELEMENT=ASLP\n",
      "   3. ELEMENT=ASTP\n",
      "   4. ELEMENT=AWBT\n",
      "   5. ELEMENT=AWDR\n",
      "   6. ELEMENT=AWND\n",
      "   7. ELEMENT=DAEV\n",
      "   8. ELEMENT=DAPR\n",
      "   9. ELEMENT=DASF\n",
      "  10. ELEMENT=DATN\n",
      "  11. ELEMENT=DATX\n",
      "  12. ELEMENT=DWPR\n",
      "  13. ELEMENT=EVAP\n",
      "  14. ELEMENT=FMTM\n",
      "  15. ELEMENT=MDPR\n",
      "  16. ELEMENT=MDSF\n",
      "  17. ELEMENT=MDTN\n",
      "  18. ELEMENT=MDTX\n",
      "  19. ELEMENT=MNPN\n",
      "  20. ELEMENT=MXPN\n",
      "\n",
      "\n",
      "Actual parquet files in YEAR=2012/ELEMENT=PGTM:\n",
      "--------------------------------------------------------------------------------\n",
      "Number of parquet files: 1\n",
      "\n",
      "File sizes:\n",
      "  1. 1b8a346935904eea8e0b774358625c41_0.snapp (  0.17 MB)\n"
     ]
    }
   ],
   "source": [
    "# List year directories in by_year\n",
    "by_year_path = f'{bucket_name}/parquet/by_year'\n",
    "year_dirs = s3.ls(by_year_path)\n",
    "\n",
    "print('BY_YEAR FILE ORGANIZATION')\n",
    "print('='*80)\n",
    "print(f'\\nTotal year directories: {len(year_dirs)}')\n",
    "print('\\nFirst 10 years:')\n",
    "for i, f in enumerate(year_dirs[:10]):\n",
    "    year = f.split('/')[-1]\n",
    "    print(f'  {i+1:2d}. {year}')\n",
    "\n",
    "print('\\nLast 10 years:')\n",
    "for i, f in enumerate(year_dirs[-10:]):\n",
    "    year = f.split('/')[-1]\n",
    "    print(f'  {i+1:2d}. {year}')\n",
    "\n",
    "# Examine one year's element directories\n",
    "sample_year = year_dirs[250]  # Recent year\n",
    "print(f'\\n\\nExploring elements in {sample_year.split(\"/\")[-1]}:')\n",
    "print('-'*80)\n",
    "element_dirs = s3.ls(sample_year)\n",
    "print(f'Total elements: {len(element_dirs)}')\n",
    "print('\\nFirst 20 elements:')\n",
    "for i, elem_dir in enumerate(element_dirs[:20]):\n",
    "    element = elem_dir.split('/')[-1]\n",
    "    print(f'  {i+1:2d}. {element}')\n",
    "\n",
    "# Look at actual parquet files in one element\n",
    "sample_element = element_dirs[20]  # PRCP is common\n",
    "print(f'\\n\\nActual parquet files in {sample_element.split(\"/\")[-2]}/{sample_element.split(\"/\")[-1]}:')\n",
    "print('-'*80)\n",
    "parquet_files = s3.ls(sample_element)\n",
    "print(f'Number of parquet files: {len(parquet_files)}')\n",
    "print('\\nFile sizes:')\n",
    "for i, pf in enumerate(parquet_files[:5]):\n",
    "    info = s3.info(pf)\n",
    "    size_mb = info['size'] / (1024 * 1024)\n",
    "    filename = pf.split('/')[-1]\n",
    "    print(f'  {i+1}. {filename[:40]:40s} ({size_mb:6.2f} MB)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:47.439416Z",
     "iopub.status.busy": "2025-10-13T23:39:47.438733Z",
     "iopub.status.idle": "2025-10-13T23:39:48.058253Z",
     "shell.execute_reply": "2025-10-13T23:39:48.057841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARQUET SCHEMA AND FIELDS\n",
      "================================================================================\n",
      "\n",
      "Reading sample file: YEAR=2012/ELEMENT=PGTM/1b8a346935904eea8e0b774358625c41_0.snapp\n",
      "\n",
      "DataFrame shape: 105,055 rows × 7 columns\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COLUMN NAMES AND DATA TYPES:\n",
      "--------------------------------------------------------------------------------\n",
      "  ID              : object           (null:   0.0%)\n",
      "  DATE            : object           (null:   0.0%)\n",
      "  DATA_VALUE      : int64            (null:   0.0%)\n",
      "  M_FLAG          : object           (null: 100.0%)\n",
      "  Q_FLAG          : object           (null: 100.0%)\n",
      "  S_FLAG          : object           (null:   0.0%)\n",
      "  OBS_TIME        : object           (null: 100.0%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FIRST 10 ROWS:\n",
      "--------------------------------------------------------------------------------\n",
      "            ID      DATE  DATA_VALUE M_FLAG Q_FLAG S_FLAG OBS_TIME\n",
      "0  FMW00040308  20120101         951   None   None      W     None\n",
      "1  FMW00040504  20120101        1350   None   None      W     None\n",
      "2  USW00013986  20120101         202   None   None      W     None\n",
      "3  USW00014605  20120101        2334   None   None      W     None\n",
      "4  USW00014702  20120101        2347   None   None      W     None\n",
      "5  USW00014707  20120101        2239   None   None      W     None\n",
      "6  USW00014710  20120101        1836   None   None      W     None\n",
      "7  USW00014719  20120101        2148   None   None      W     None\n",
      "8  USW00014736  20120101        1838   None   None      W     None\n",
      "9  USW00014747  20120101        1858   None   None      W     None\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "BASIC STATISTICS (numeric columns):\n",
      "--------------------------------------------------------------------------------\n",
      "          DATA_VALUE\n",
      "count  105055.000000\n",
      "mean     1354.428004\n",
      "std       555.295994\n",
      "min         0.000000\n",
      "25%      1108.000000\n",
      "50%      1412.000000\n",
      "75%      1654.000000\n",
      "max      2359.000000\n"
     ]
    }
   ],
   "source": [
    "# Read a sample parquet file to examine schema and fields\n",
    "sample_file = parquet_files[0]  # Use first actual parquet file\n",
    "print('PARQUET SCHEMA AND FIELDS')\n",
    "print('='*80)\n",
    "year = sample_file.split('/')[-3]\n",
    "element = sample_file.split('/')[-2]\n",
    "filename = sample_file.split('/')[-1]\n",
    "print(f'\\nReading sample file: {year}/{element}/{filename[:40]}\\n')\n",
    "\n",
    "with s3.open(sample_file, 'rb') as f:\n",
    "    df_sample = pd.read_parquet(f)\n",
    "\n",
    "print(f'DataFrame shape: {df_sample.shape[0]:,} rows × {df_sample.shape[1]} columns')\n",
    "print('\\n' + '-'*80)\n",
    "print('COLUMN NAMES AND DATA TYPES:')\n",
    "print('-'*80)\n",
    "for col, dtype in df_sample.dtypes.items():\n",
    "    non_null = df_sample[col].notna().sum()\n",
    "    pct_null = (1 - non_null / len(df_sample)) * 100\n",
    "    print(f'  {col:15s} : {str(dtype):15s}  (null: {pct_null:5.1f}%)')\n",
    "\n",
    "print('\\n' + '-'*80)\n",
    "print('FIRST 10 ROWS:')\n",
    "print('-'*80)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "print(df_sample.head(10))\n",
    "\n",
    "print('\\n' + '-'*80)\n",
    "print('BASIC STATISTICS (numeric columns):')\n",
    "print('-'*80)\n",
    "print(df_sample.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:48.060382Z",
     "iopub.status.busy": "2025-10-13T23:39:48.060133Z",
     "iopub.status.idle": "2025-10-13T23:39:48.123003Z",
     "shell.execute_reply": "2025-10-13T23:39:48.122679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CHARACTERISTICS\n",
      "================================================================================\n",
      "\n",
      "Unique stations: 889\n",
      "\n",
      "Top 10 stations by observation count:\n",
      "--------------------------------------------------------------------------------\n",
      "  USW00094225: 366 observations\n",
      "  PSW00040309: 366 observations\n",
      "  USW00014858: 366 observations\n",
      "  USW00025309: 366 observations\n",
      "  VQW00011624: 366 observations\n",
      "  USW00026523: 366 observations\n",
      "  USW00025331: 366 observations\n",
      "  USW00025335: 366 observations\n",
      "  USW00053853: 365 observations\n",
      "  USW00053850: 365 observations\n",
      "\n",
      "\n",
      "Unique dates: 366\n",
      "Date range: 20120101 to 20121231\n",
      "\n",
      "\n",
      "Data Value Statistics:\n",
      "  Min: 0\n",
      "  Max: 2359\n",
      "  Mean: 1354.43\n",
      "  Median: 1412.00\n",
      "\n",
      "\n",
      "Flag Columns:\n",
      "\n",
      "S_FLAG:\n",
      "W    105055\n",
      "Name: S_FLAG, dtype: int64\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MEMORY USAGE:\n",
      "--------------------------------------------------------------------------------\n",
      "Total memory usage: 27.15 MB\n",
      "\n",
      "Per column:\n",
      "  Index           :   0.00 MB (  0.0%)\n",
      "  ID              :   6.81 MB ( 25.1%)\n",
      "  DATE            :   6.51 MB ( 24.0%)\n",
      "  DATA_VALUE      :   0.80 MB (  3.0%)\n",
      "  M_FLAG          :   2.40 MB (  8.9%)\n",
      "  Q_FLAG          :   2.40 MB (  8.9%)\n",
      "  S_FLAG          :   5.81 MB ( 21.4%)\n",
      "  OBS_TIME        :   2.40 MB (  8.9%)\n"
     ]
    }
   ],
   "source": [
    "# Examine unique values and data characteristics\n",
    "print('DATA CHARACTERISTICS')\n",
    "print('='*80)\n",
    "\n",
    "# Check station data (column name is 'ID' not 'id')\n",
    "if 'ID' in df_sample.columns:\n",
    "    print(f'\\nUnique stations: {df_sample[\"ID\"].nunique():,}')\n",
    "    print('\\nTop 10 stations by observation count:')\n",
    "    print('-'*80)\n",
    "    top_stations = df_sample['ID'].value_counts().head(10)\n",
    "    for station, count in top_stations.items():\n",
    "        print(f'  {station}: {count:,} observations')\n",
    "\n",
    "# Check unique dates\n",
    "if 'DATE' in df_sample.columns:\n",
    "    print(f'\\n\\nUnique dates: {df_sample[\"DATE\"].nunique():,}')\n",
    "    print('Date range:', df_sample['DATE'].min(), 'to', df_sample['DATE'].max())\n",
    "\n",
    "# Check data value range\n",
    "if 'DATA_VALUE' in df_sample.columns:\n",
    "    print(f'\\n\\nData Value Statistics:')\n",
    "    print(f'  Min: {df_sample[\"DATA_VALUE\"].min()}')\n",
    "    print(f'  Max: {df_sample[\"DATA_VALUE\"].max()}')\n",
    "    print(f'  Mean: {df_sample[\"DATA_VALUE\"].mean():.2f}')\n",
    "    print(f'  Median: {df_sample[\"DATA_VALUE\"].median():.2f}')\n",
    "\n",
    "# Check flags\n",
    "flag_cols = [col for col in df_sample.columns if 'FLAG' in col]\n",
    "if flag_cols:\n",
    "    print(f'\\n\\nFlag Columns:')\n",
    "    for flag_col in flag_cols:\n",
    "        non_null = df_sample[flag_col].notna().sum()\n",
    "        if non_null > 0:\n",
    "            print(f'\\n{flag_col}:')\n",
    "            print(df_sample[flag_col].value_counts().head(5))\n",
    "\n",
    "# Show memory usage\n",
    "print('\\n' + '-'*80)\n",
    "print('MEMORY USAGE:')\n",
    "print('-'*80)\n",
    "mem_usage = df_sample.memory_usage(deep=True)\n",
    "total_mb = mem_usage.sum() / (1024 * 1024)\n",
    "print(f'Total memory usage: {total_mb:.2f} MB')\n",
    "print('\\nPer column:')\n",
    "for col, mem in mem_usage.items():\n",
    "    mem_mb = mem / (1024 * 1024)\n",
    "    pct = (mem / mem_usage.sum()) * 100\n",
    "    print(f'  {col:15s} : {mem_mb:6.2f} MB ({pct:5.1f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:39:48.124560Z",
     "iopub.status.busy": "2025-10-13T23:39:48.124469Z",
     "iopub.status.idle": "2025-10-13T23:39:48.332472Z",
     "shell.execute_reply": "2025-10-13T23:39:48.331891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PARTITIONING SCHEME SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ Two-level hierarchical partitioning: YEAR → ELEMENT\n",
      "\n",
      "Level 1 - Years:\n",
      "  - Total years: 264\n",
      "  - Range: 1750 to 2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Level 2 - Elements (weather measurement types):\n",
      "  - Average elements per year: ~44\n",
      "  - Sample year (YEAR=2012): 98 elements\n",
      "\n",
      "Common core elements available:\n",
      "  ✓ ELEMENT=PRCP\n",
      "  ✓ ELEMENT=TMAX\n",
      "  ✓ ELEMENT=TMIN\n",
      "  ✓ ELEMENT=SNOW\n",
      "  ✓ ELEMENT=TAVG\n",
      "\n",
      "\n",
      "Total data organization:\n",
      "  - 264 year directories\n",
      "  - ~11,616 year/element partitions\n",
      "  - Multiple parquet files per partition (e.g., 1 files in sampled partition)\n",
      "  - File format: Snappy-compressed Parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Typical file size: ~0.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Analyze partitioning scheme summary\n",
    "print('\\n' + '='*80)\n",
    "print('PARTITIONING SCHEME SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "# Extract years from directory names\n",
    "years = []\n",
    "for year_dir in year_dirs:\n",
    "    year_name = year_dir.split('/')[-1]\n",
    "    if year_name.startswith('YEAR='):\n",
    "        years.append(year_name.replace('YEAR=', ''))\n",
    "\n",
    "print(f'\\n✓ Two-level hierarchical partitioning: YEAR → ELEMENT')\n",
    "print(f'\\nLevel 1 - Years:')\n",
    "print(f'  - Total years: {len(years)}')\n",
    "print(f'  - Range: {years[0]} to {years[-1]}')\n",
    "\n",
    "# Sample a few years to get average element count\n",
    "sample_years = [year_dirs[i] for i in [50, 150, 250]]\n",
    "element_counts = []\n",
    "for sy in sample_years:\n",
    "    element_counts.append(len(s3.ls(sy)))\n",
    "\n",
    "print(f'\\nLevel 2 - Elements (weather measurement types):')\n",
    "print(f'  - Average elements per year: ~{int(np.mean(element_counts))}')\n",
    "print(f'  - Sample year ({sample_year.split(\"/\")[-1]}): {len(element_dirs)} elements')\n",
    "\n",
    "# Show some common elements\n",
    "common_elements = ['ELEMENT=PRCP', 'ELEMENT=TMAX', 'ELEMENT=TMIN', 'ELEMENT=SNOW', 'ELEMENT=TAVG']\n",
    "available_elements = [e.split('/')[-1] for e in element_dirs]\n",
    "print(f'\\nCommon core elements available:')\n",
    "for ce in common_elements:\n",
    "    status = '✓' if ce in available_elements else '✗'\n",
    "    print(f'  {status} {ce}')\n",
    "\n",
    "print(f'\\n\\nTotal data organization:')\n",
    "print(f'  - {len(year_dirs)} year directories')\n",
    "print(f'  - ~{len(year_dirs) * int(np.mean(element_counts)):,} year/element partitions')\n",
    "print(f'  - Multiple parquet files per partition (e.g., {len(parquet_files)} files in sampled partition)')\n",
    "print(f'  - File format: Snappy-compressed Parquet')\n",
    "print(f'  - Typical file size: ~{np.mean([s3.info(pf)[\"size\"] for pf in parquet_files[:5]]) / (1024*1024):.1f} MB')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
