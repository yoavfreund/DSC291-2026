{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring NOAA GHCN Data on S3\n",
    "\n",
    "## Global Historical Climatology Network (GHCN)\n",
    "\n",
    "This notebook explores the **NOAA Global Historical Climatology Network (GHCN)** dataset stored on AWS S3.\n",
    "\n",
    "### Dataset Information:\n",
    "- **S3 Bucket**: `s3://noaa-ghcn-pds/`\n",
    "- **Registry**: [AWS Open Data Registry](https://registry.opendata.aws/noaa-ghcn/)\n",
    "- **Time Period**: 1750 - Present (275+ years!)\n",
    "- **Format**: Parquet and CSV.GZ\n",
    "- **Size**: Billions of observations worldwide\n",
    "\n",
    "### Goals:\n",
    "1. List available years and data organization\n",
    "2. Understand data distribution across time\n",
    "3. Explore geographic coverage (stations worldwide)\n",
    "4. Sample data to understand structure\n",
    "5. Analyze metadata and completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install required packages:\n",
    "```bash\n",
    "pip install s3fs boto3 pandas pyarrow dask matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print('\u2713 Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to S3 Bucket\n",
    "\n",
    "The NOAA GHCN data is publicly available (no credentials needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 filesystem (anonymous access)\n",
    "print('Connecting to S3...')\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# NOAA GHCN bucket\n",
    "bucket_name = 'noaa-ghcn-pds'\n",
    "print(f'\u2713 Connected to S3 bucket: {bucket_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Bucket Structure\n",
    "\n",
    "Let's see how the data is organized in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List top-level directories\n",
    "print('Top-level directories in the bucket:')\n",
    "print('='*60)\n",
    "top_level = s3.ls(bucket_name)\n",
    "for item in top_level:\n",
    "    print(f'  {item}')\n",
    "\n",
    "print(f'\\nTotal items: {len(top_level)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore parquet directory\n",
    "print('Exploring parquet data organization:')\n",
    "print('='*60)\n",
    "parquet_dirs = s3.ls(f'{bucket_name}/parquet/')\n",
    "for item in parquet_dirs:\n",
    "    print(f'  {item}')\n",
    "    \n",
    "    # Get size info if it's a directory\n",
    "    if item.endswith('/'):\n",
    "        subfiles = s3.ls(item)\n",
    "        print(f'    \u2514\u2500 Contains {len(subfiles)} items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discover Available Years\n",
    "\n",
    "Let's find out what years are available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List years in the by_year directory\n",
    "print('Discovering available years...')\n",
    "year_dirs = s3.ls(f'{bucket_name}/parquet/by_year/')\n",
    "\n",
    "# Extract years from directory names\n",
    "years = []\n",
    "for year_dir in year_dirs:\n",
    "    # Extract YEAR=YYYY from path\n",
    "    if 'YEAR=' in year_dir:\n",
    "        year = int(year_dir.split('YEAR=')[1].strip('/'))\n",
    "        years.append(year)\n",
    "\n",
    "years = sorted(years)\n",
    "\n",
    "print('='*60)\n",
    "print(f'Total years available: {len(years)}')\n",
    "print(f'Year range: {min(years)} - {max(years)}')\n",
    "print(f'\\nFirst 10 years: {years[:10]}')\n",
    "print(f'Last 10 years: {years[-10:]}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal distribution\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "fig.suptitle('NOAA GHCN: Temporal Coverage', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Full timeline\n",
    "axes[0].bar(years, [1]*len(years), width=1.0, color='steelblue', edgecolor='none')\n",
    "axes[0].set_xlabel('Year', fontsize=12)\n",
    "axes[0].set_ylabel('Data Available', fontsize=12)\n",
    "axes[0].set_title(f'Complete Timeline: {min(years)} - {max(years)} ({len(years)} years)', fontsize=13)\n",
    "axes[0].set_xlim(min(years)-5, max(years)+5)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Decade bins\n",
    "decade_bins = np.arange(min(years)//10*10, max(years)+10, 10)\n",
    "decade_counts, _ = np.histogram(years, bins=decade_bins)\n",
    "decade_labels = [f'{int(d)}s' for d in decade_bins[:-1]]\n",
    "\n",
    "axes[1].bar(decade_bins[:-1], decade_counts, width=9, color='orange', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Decade', fontsize=12)\n",
    "axes[1].set_ylabel('Years Available', fontsize=12)\n",
    "axes[1].set_title('Data Availability by Decade', fontsize=13)\n",
    "axes[1].set_xticks(decade_bins[:-1])\n",
    "axes[1].set_xticklabels(decade_labels, rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nDecade summary:')\n",
    "for decade, count in zip(decade_bins[:-1], decade_counts):\n",
    "    print(f'  {int(decade)}s: {int(count)} years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Data Structure\n",
    "\n",
    "Let's load a sample of data from a recent year to understand the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample from 2023\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "sample_year = 2023\n",
    "print(f'Loading sample data from year {sample_year}...')\n",
    "\n",
    "# Get files for this year\n",
    "year_path = f'{bucket_name}/parquet/by_year/YEAR={sample_year}/'\n",
    "year_files = s3.ls(year_path)\n",
    "\n",
    "print(f'Files available for {sample_year}: {len(year_files)}')\n",
    "print(f'First few files:')\n",
    "for f in year_files[:5]:\n",
    "    size = s3.size(f) / (1024**2)  # Convert to MB\n",
    "    print(f'  {f.split(\"/\")[-1]}: {size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a small sample\n",
    "print(f'\\nReading sample from {sample_year}...')\n",
    "sample_file = year_files[0]\n",
    "\n",
    "# Open file with S3\n",
    "with s3.open(sample_file, 'rb') as f:\n",
    "    # Read parquet file\n",
    "    table = pq.read_table(f)\n",
    "    df_sample = table.to_pandas()\n",
    "\n",
    "print(f'\u2713 Loaded {len(df_sample):,} rows')\n",
    "print(f'\\nDataFrame info:')\n",
    "print(df_sample.info())\n",
    "print(f'\\nFirst few rows:')\n",
    "df_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data columns and types\n",
    "print('='*60)\n",
    "print('Data Structure Summary')\n",
    "print('='*60)\n",
    "print(f'Columns: {list(df_sample.columns)}')\n",
    "print(f'\\nSample statistics:')\n",
    "print(df_sample.describe())\n",
    "\n",
    "# Check for unique values in key columns\n",
    "print(f'\\nUnique stations in sample: {df_sample[\"ID\"].nunique():,}')\n",
    "if 'ELEMENT' in df_sample.columns:\n",
    "    print(f'Unique measurement types (ELEMENT): {df_sample[\"ELEMENT\"].nunique()}')\n",
    "    print(f'Top measurement types:')\n",
    "    print(df_sample['ELEMENT'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geographic Distribution (Stations)\n",
    "\n",
    "Let's explore where the weather stations are located worldwide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Station metadata (if available)\n",
    "print('Looking for station metadata...')\n",
    "csv_files = s3.ls(f'{bucket_name}/csv/')\n",
    "print(f'CSV directory contents:')\n",
    "for item in csv_files[:10]:\n",
    "    print(f'  {item}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique stations across recent years\n",
    "print('Analyzing station distribution across years...')\n",
    "recent_years = [2020, 2021, 2022, 2023, 2024]\n",
    "station_counts = {}\n",
    "\n",
    "for year in recent_years:\n",
    "    try:\n",
    "        year_path = f'{bucket_name}/parquet/by_year/YEAR={year}/'\n",
    "        files = s3.ls(year_path)\n",
    "        \n",
    "        if files:\n",
    "            # Read first file to get station count estimate\n",
    "            with s3.open(files[0], 'rb') as f:\n",
    "                table = pq.read_table(f, columns=['ID'])\n",
    "                df_temp = table.to_pandas()\n",
    "                station_counts[year] = df_temp['ID'].nunique()\n",
    "                \n",
    "        print(f'  {year}: {station_counts.get(year, \"N/A\")} unique stations (sample)')\n",
    "    except Exception as e:\n",
    "        print(f'  {year}: Error - {str(e)[:50]}')\n",
    "\n",
    "# Visualize\n",
    "if station_counts:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    years_list = list(station_counts.keys())\n",
    "    counts_list = list(station_counts.values())\n",
    "    \n",
    "    ax.bar(years_list, counts_list, color='teal', alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Year', fontsize=12)\n",
    "    ax.set_ylabel('Number of Unique Stations (sample)', fontsize=12)\n",
    "    ax.set_title('Weather Stations by Year', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Volume Analysis\n",
    "\n",
    "Estimate the size and number of observations across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample multiple years to estimate data volume\n",
    "print('Estimating data volume across years...')\n",
    "sample_years = [1900, 1950, 2000, 2010, 2015, 2020, 2023]\n",
    "volume_info = []\n",
    "\n",
    "for year in sample_years:\n",
    "    try:\n",
    "        year_path = f'{bucket_name}/parquet/by_year/YEAR={year}/'\n",
    "        files = s3.ls(year_path)\n",
    "        \n",
    "        # Calculate total size\n",
    "        total_size = sum(s3.size(f) for f in files) / (1024**3)  # GB\n",
    "        \n",
    "        volume_info.append({\n",
    "            'Year': year,\n",
    "            'Files': len(files),\n",
    "            'Size_GB': total_size\n",
    "        })\n",
    "        print(f'  {year}: {len(files)} files, {total_size:.2f} GB')\n",
    "    except Exception as e:\n",
    "        print(f'  {year}: Not available or error')\n",
    "\n",
    "# Create DataFrame and visualize\n",
    "if volume_info:\n",
    "    df_volume = pd.DataFrame(volume_info)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('Data Volume Growth Over Time', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Number of files\n",
    "    axes[0].plot(df_volume['Year'], df_volume['Files'], marker='o', linewidth=2, markersize=8, color='blue')\n",
    "    axes[0].set_xlabel('Year', fontsize=12)\n",
    "    axes[0].set_ylabel('Number of Files', fontsize=12)\n",
    "    axes[0].set_title('Files per Year', fontsize=13)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Data size\n",
    "    axes[1].plot(df_volume['Year'], df_volume['Size_GB'], marker='s', linewidth=2, markersize=8, color='red')\n",
    "    axes[1].set_xlabel('Year', fontsize=12)\n",
    "    axes[1].set_ylabel('Data Size (GB)', fontsize=12)\n",
    "    axes[1].set_title('Storage Size per Year', fontsize=13)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nSummary:')\n",
    "    print(df_volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "This notebook explored the NOAA GHCN dataset on S3:\n",
    "\n",
    "\u2705 **Temporal Coverage**: 275+ years of historical weather data\n",
    "\u2705 **Data Organization**: Parquet files organized by year and station\n",
    "\u2705 **Geographic Coverage**: Thousands of weather stations worldwide\n",
    "\u2705 **Data Volume**: Growing dataset with recent years containing most data\n",
    "\u2705 **Measurement Types**: Multiple weather elements (TMAX, TMIN, PRCP, etc.)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Load specific years** for detailed analysis\n",
    "2. **Analyze trends** over time (climate change indicators)\n",
    "3. **Geographic patterns** by region/continent\n",
    "4. **Seasonal analysis** within years\n",
    "5. **Compare** different weather elements\n",
    "6. **Quality control** analysis using flags\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [NOAA GHCN Documentation](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily)\n",
    "- [AWS Open Data Registry](https://registry.opendata.aws/noaa-ghcn/)\n",
    "- [Data Format Description](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}